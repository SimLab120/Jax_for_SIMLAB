{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b861b7c2-9f23-4070-afaa-731063608005",
   "metadata": {},
   "source": [
    "# JAX Introduction: Arrays, Differentiation, and JIT\n",
    "\n",
    "Welcome! This notebook is designed to help you understand what JAX is and why it is important for modern scientific computing and machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is JAX?\n",
    "\n",
    "JAX is a numerical computing library from Google that brings together:\n",
    "- **NumPy-like** array programming (but faster and more flexible)\n",
    "- **Automatic differentiation (autograd)**\n",
    "- **Just-In-Time (JIT) compilation** for performance\n",
    "- Built-in support for **parallelism** (across CPUs, GPUs, TPUs)\n",
    "\n",
    "> JAX is especially popular in research for its clean, functional programming style and speed.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. JAX vs NumPy: Arrays and API\n",
    "\n",
    "JAX arrays (`jax.numpy` or `jnp`) behave almost exactly like NumPy arrays, but with some crucial differences:\n",
    "\n",
    "- Immutable (cannot be changed in-place)\n",
    "- Operate on CPU, GPU, or TPU automatically\n",
    "- Compatible with JAX's transformations (`grad`, `jit`, etc.)\n",
    "\n",
    "### Exercise 1: Basic Array Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c211c3-9a0d-4083-891b-9f5926f158bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fdbf22-c4bd-4279-bb8c-de2c4ff44a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1. 2. 3.]\n",
      "x + 2: [3. 4. 5.]\n",
      "sin(x): [0.84147096 0.9092974  0.14112   ]\n",
      "Sum: 6.0\n",
      "Error: JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://docs.jax.dev/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
     ]
    }
   ],
   "source": [
    "# Create an array\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "print(\"x:\", x)\n",
    "\n",
    "# Elementwise operations\n",
    "print(\"x + 2:\", x + 2)\n",
    "print(\"sin(x):\", jnp.sin(x))\n",
    "print(\"Sum:\", x.sum())\n",
    "\n",
    "# JAX arrays are immutable!\n",
    "try:\n",
    "    x[0] = 100  # This will raise an error\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2993a-061b-4d58-814e-8b341a92638c",
   "metadata": {},
   "source": [
    "### Mutating or Replacing Parts of an Array in JAX\n",
    "\n",
    "JAX arrays are **immutable**—you cannot change their contents in-place like NumPy arrays.  \n",
    "Instead, you use functions that **return a new array** with your changes.\n",
    "\n",
    "#### Example: Using `jax.numpy`'s `.at[].set()` and `.at[].add()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7b4b4e1-73d2-4be4-ad5b-2956c951fc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: [1 2 3 4 5]\n",
      "After set: [ 1  2 99  4  5]\n",
      "After add: [ 1 12  3 14  5]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Set the value at index 2 to 99\n",
    "y = x.at[2].set(99)\n",
    "print(\"Original x:\", x)\n",
    "print(\"After set:\", y)  # [ 1  2 99  4  5]\n",
    "\n",
    "# Add 10 to indices 1 and 3 (correct way: pass a 1D array of indices)\n",
    "ind = jnp.array([1, 3])\n",
    "z = x.at[ind].add(10)\n",
    "print(\"After add:\", z)  # [ 1 12  3 14  5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09673b-8d00-481c-aaff-9b5409a72a3f",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Automatic Differentiation (`jax.grad`)\n",
    "\n",
    "JAX can compute gradients of Python functions automatically, even for complicated functions.\n",
    "\n",
    "### Example: Gradient of a Scalar Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c9f9e9-130b-4ca5-8fba-7db789f462ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0) = 10.0\n",
      "df/dx at x=2.0 = 7.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2 + 3.0 * x\n",
    "\n",
    "dfdx = jax.grad(f)\n",
    "\n",
    "print(\"f(2.0) =\", f(2.0))\n",
    "print(\"df/dx at x=2.0 =\", dfdx(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a440e0b-cef4-4fd2-abbe-a6307461479f",
   "metadata": {},
   "source": [
    "### Exercise 2: Try It Yourself\n",
    "\n",
    "- Define a function `g(x) = sin(x) + x**3`\n",
    "- Use `jax.grad` to compute its derivative at `x=1.0`\n",
    "- Print both the function value and its derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287983fe-da47-458c-a5d1-e7104f835912",
   "metadata": {},
   "source": [
    "## 4. JIT Compilation for Speed (`jax.jit`)\n",
    "\n",
    "JAX can \"compile\" your functions to run much faster using XLA, especially for repeated calls.\n",
    "\n",
    "### Example: JIT-accelerated function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f2b58ce-0305-4153-a305-c6315435f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are results close? True\n",
      "Eager (not-JIT) execution:\n",
      "271 μs ± 4.19 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "JIT-compiled execution (after compilation):\n",
      "182 μs ± 19.4 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import numpy as np \n",
    "# Define a normalization function\n",
    "def norm(X):\n",
    "    X = X - X.mean(0)\n",
    "    return X / X.std(0)\n",
    "\n",
    "# JIT-compiled version\n",
    "norm_compiled = jax.jit(norm)\n",
    "\n",
    "# Prepare a large random array\n",
    "np.random.seed(1701)\n",
    "X = jnp.array(np.random.rand(1000, 10))\n",
    "\n",
    "# Check correctness\n",
    "print(\"Are results close?\", np.allclose(norm(X), norm_compiled(X), atol=1E-6))\n",
    "\n",
    "# Time the uncompiled (eager) version\n",
    "print(\"Eager (not-JIT) execution:\")\n",
    "%timeit norm(X).block_until_ready()  # block_until_ready() ensures accurate timing\n",
    "\n",
    "# Time the compiled (JIT) version (first call includes compilation)\n",
    "print(\"JIT-compiled execution (after compilation):\")\n",
    "_ = norm_compiled(X).block_until_ready()  # warm-up to compile\n",
    "%timeit norm_compiled(X).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74ad3d-0885-4086-983c-caf406db6fb6",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "### Note:\n",
    "- The **first call** to a JIT function might be slow (compilation), but all later calls are very fast!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcedf16-3166-44a4-b2d7-d75b59121148",
   "metadata": {},
   "source": [
    "# JAX: Understanding `vmap` and `pmap`\n",
    "\n",
    "JAX provides **powerful tools for vectorization and parallelization**:  \n",
    "- `vmap` for **automatic batching** on a single device (CPU, GPU, or TPU)\n",
    "- `pmap` for **parallelization across multiple devices** (multiple GPUs or TPUs)\n",
    "\n",
    "This notebook gives a detailed explanation and practical examples of both.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `vmap`: Automatic Vectorization (Batching)\n",
    "\n",
    "### What does `vmap` do?\n",
    "\n",
    "`vmap` takes a function written for **single examples** and automatically lifts it to operate on **batches of examples**—without writing explicit loops.\n",
    "\n",
    "- **Why use it?**: Vectorized code is faster and easier to read than for-loops.\n",
    "- **Analogy**: Like “numpy broadcasting on steroids,” but works for any function (even those with JAX transforms).\n",
    "\n",
    "### Basic Example: Computing Gradients for a Batch\n",
    "\n",
    "Suppose you want to compute the gradient of a function for many inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7818ab-7e00-40e9-b1f3-7093bb9a0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual loop: [-1.  1.  3.  5.  7.]\n",
      "vmap: [-1.  1.  3.  5.  7.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 3.0 * x\n",
    "\n",
    "dfdx = jax.grad(f)\n",
    "x_batch = jnp.linspace(-2, 2, 5)\n",
    "\n",
    "# Without vmap: would require a Python for-loop\n",
    "grads = jnp.array([dfdx(x) for x in x_batch])\n",
    "\n",
    "# With vmap: no loop needed!\n",
    "vmap_grads = jax.vmap(dfdx)(x_batch)\n",
    "\n",
    "print(\"Manual loop:\", grads)\n",
    "print(\"vmap:\", vmap_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c44f3d-fb82-476f-970f-a2e37222d315",
   "metadata": {},
   "source": [
    "### How does batching work?\n",
    "\n",
    "- By default, `vmap` applies the function along the **first axis** of array arguments (\"leading batch dimension\").\n",
    "- You can control which axis is batched via the `in_axes` and `out_axes` arguments.\n",
    "\n",
    "#### Example with multiple arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9073075-ba36-4d8f-82d4-f12f73269fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  6 11 18]\n"
     ]
    }
   ],
   "source": [
    "def my_func(x, y):\n",
    "    return x * y + 2\n",
    "\n",
    "xs = jnp.arange(5)\n",
    "ys = jnp.arange(5)\n",
    "\n",
    "# vmap over both xs and ys simultaneously\n",
    "result = jax.vmap(my_func)(xs, ys)\n",
    "print(result)  # [2 3 6 11 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64cf8e3-6378-4d84-9331-f37da33e94d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 12 22 32 42]\n"
     ]
    }
   ],
   "source": [
    "# vmap over only xs, keep y scalar\n",
    "result = jax.vmap(my_func, in_axes=(0, None))(xs, 10)\n",
    "print(result)  # [2 12 22 32 42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc15157-d489-4387-9aa1-c11177531790",
   "metadata": {},
   "source": [
    "### Advanced: Nested `vmap` for 2D batching\n",
    "\n",
    "Sometimes you want to apply a function to **all pairs** from two arrays, for example:  \n",
    "- Given vectors `A` (length M) and `B` (length N), compute `add(a, b)` for every combination of `a` in `A` and `b` in `B`, producing an M×N output.\n",
    "\n",
    "You could do this with nested Python loops, but JAX's nested `vmap` lets you express this efficiently and concisely:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91ee743f-3b5a-443d-9057-b730f9cf07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "A = jnp.arange(3)    # shape (3,)\n",
    "B = jnp.arange(4)    # shape (4,)\n",
    "\n",
    "# First vmap: loops over elements of A, keeps B fixed each time\n",
    "# Second vmap: loops over elements of B, keeps an element of A fixed\n",
    "batched_add = jax.vmap(\n",
    "    jax.vmap(add, in_axes=(None, 0)),  # vectorize over B (y): x fixed, y varies\n",
    "    in_axes=(0, None)                  # vectorize over A (x): x varies, y fixed\n",
    ")\n",
    "\n",
    "result = batched_add(A, B)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db526433-8290-4e71-9dd9-82a2ff82b93a",
   "metadata": {},
   "source": [
    "#### How does this work?\n",
    "\n",
    "- The **inner vmap** (`in_axes=(None, 0)`) says: \"For a fixed `x`, apply `add(x, y)` for all `y` in `B`\" (vectorizing over `B`).\n",
    "- The **outer vmap** (`in_axes=(0, None)`) says: \"For all `x` in `A`, apply the inner vmap with that `x` and all of `B`\".\n",
    "\n",
    "This gives you a 2D array where `result[i, j] = A[i] + B[j]` for all `i, j`.\n",
    "\n",
    "#### Generalization\n",
    "\n",
    "You can nest `vmap` as many times as needed, enabling efficient, readable code for batched/broadcasted computations without writing explicit loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ecfb86-f5ef-412b-997a-996b2856dcc2",
   "metadata": {},
   "source": [
    "## 2. `pmap`: Parallelization Across Devices\n",
    "\n",
    "### What does `pmap` do?\n",
    "\n",
    "`pmap` transforms a function for **parallel execution across multiple devices** (e.g., multiple GPUs/TPUs in a single machine).\n",
    "\n",
    "- **Use case**: Large-scale parallelism, model/data parallel training, hardware acceleration.\n",
    "- **Not needed** for single-device code—use `vmap` instead!\n",
    "\n",
    "### Basic Example: Parallel addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec3d74f-a3ac-4ba2-8344-253ac3aca19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device count: 1\n",
      "Parallel result: [1]\n"
     ]
    }
   ],
   "source": [
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "# Simulate inputs for 2 devices\n",
    "x = jnp.arange(jax.device_count())\n",
    "print(\"Device count:\", jax.device_count())\n",
    "\n",
    "# pmap distributes computation across devices\n",
    "y = jax.pmap(add_one)(x)\n",
    "print(\"Parallel result:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fad5e4-01aa-495f-bcd1-1fbf97c37f5f",
   "metadata": {},
   "source": [
    "**On a machine with 4 GPUs, this will run on all 4 in parallel.**\n",
    "\n",
    "### Key points about `pmap`:\n",
    "\n",
    "- The **input array must be split** so that each device gets one chunk. Example: if you have 4 devices, input shape should have leading dimension 4.\n",
    "- Output is a \"sharded\" DeviceArray: one chunk per device.\n",
    "- JAX automatically handles communication between devices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc34a2-b913-4a08-936d-625a5f17fbce",
   "metadata": {},
   "source": [
    "\n",
    "### Advanced: Collective operations with `pmap`\n",
    "\n",
    "`pmap` supports device communication primitives (e.g., `jax.lax.pmean` for averaging):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7dd9d52-4b66-4765-86d8-d594b83a4809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def mean_across_devices(x):\n",
    "    # Compute mean of x across devices\n",
    "    return jax.lax.pmean(x, axis_name='i')\n",
    "\n",
    "x = jnp.arange(jax.device_count()).astype(float)\n",
    "out = jax.pmap(mean_across_devices, axis_name='i')(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415dd5fe-3d60-4b4c-841a-6020bffa4401",
   "metadata": {},
   "source": [
    "\n",
    "## `vmap` vs. `pmap`: When to use which?\n",
    "\n",
    "| Feature       | `vmap`                      | `pmap`                     |\n",
    "|---------------|-----------------------------|----------------------------|\n",
    "| Batching      | Yes (single device)         | Yes (across devices)       |\n",
    "| Parallelism   | Implicit SIMD, single device| Explicit, multi-device     |\n",
    "| Use case      | Data/model batching         | Distributed training/inference |\n",
    "| Syntax        | Simple, works like a loop   | Needs device-aware shapes  |\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment\n",
    "\n",
    "1. **Write a function** that computes the norm of a vector. Use `vmap` to apply it to a batch of vectors.\n",
    "2. **If you have multiple GPUs/TPUs**, write a function that multiplies by 2 and use `pmap` to apply over all devices.\n",
    "3. **(Bonus)** Try using `vmap` and `grad` together to compute gradients for a batch of inputs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a515f8-7a05-4a90-a4ef-cb52875419d9",
   "metadata": {},
   "source": [
    "# Pytrees and Pure Functional Programming in JAX: An Intuitive Introduction\n",
    "\n",
    "JAX is designed for high-performance, reliable, and composable scientific computing. Two key ideas make this possible:\n",
    "\n",
    "- **Pytrees**: Flexible, tree-like data structures.\n",
    "- **Pure Functional Programming**: Functions with no side effects.\n",
    "\n",
    "Let's understand these concepts in an intuitive, practical way.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is a PyTree?\n",
    "\n",
    "A **pytree** is any nested structure of lists, tuples, dicts, and other containers (including custom ones), where the \"leaves\" are regular JAX arrays or scalars.\n",
    "\n",
    "**Think of it like a tree made of Python containers, with arrays at the ends.**\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b19161b-fc51-4fe7-b0ed-c4db60fe2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple pytree: a list of arrays\n",
    "a = [jnp.ones(3), jnp.zeros(2)]\n",
    "\n",
    "# Nested pytree: dicts, lists, tuples, arrays\n",
    "b = {'params': (jnp.ones(2), [jnp.zeros(3), jnp.eye(2)]), 'lr': 0.01}\n",
    "\n",
    "# Even more complex\n",
    "c = (jnp.array(1.0), [{'a': jnp.array([1, 2])}, (jnp.zeros(1),)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41317723-fb17-4d27-bffd-c25177309380",
   "metadata": {},
   "source": [
    "JAX can **recursively traverse** these objects, find all the arrays, and do things with them (mapping, flattening, etc).\n",
    "\n",
    "### Why Pytrees?\n",
    "\n",
    "- **Flexibility**: You can organize your model parameters, optimizer states, batches, etc. however you like.\n",
    "- **Compatibility**: JAX's core APIs work with any pytree structure, not just flat arrays.\n",
    "- **Ease of Use**: You can use familiar Python containers, no need for new data types.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8dc52-ef43-4656-acaa-1415e5f351b8",
   "metadata": {},
   "source": [
    "## 2. Pure Functional Programming\n",
    "\n",
    "JAX encourages **pure functions**: given the same inputs, always return the same outputs, with **no side effects** (no modifying globals, printing, or changing inputs in-place).\n",
    "\n",
    "### Why does JAX care about purity?\n",
    "\n",
    "- **Transformations**: JAX can only do things like `jit`, `grad`, `vmap`, etc. safely if functions have no side effects.\n",
    "- **Reproducibility**: Pure functions are easier to debug and test.\n",
    "- **Parallelism**: Pure functions can be run in parallel safely.\n",
    "\n",
    "### Example: Pure vs. Impure\n",
    "\n",
    "## 2.1. Examples: Pure vs. Impure Functions in Python (and JAX)\n",
    "\n",
    "A **pure function** always returns the same output for the same input, and does not cause (or depend upon) any side effects.  \n",
    "An **impure function** might change things outside itself, or depend on external/global state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0dd8ad0-227c-4459-870e-16c9f7af26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Example 1: Mathematical function (Pure vs. Impure)\n",
    "\n",
    "\n",
    "# Pure: always gives same output for same inputs, no side effects\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "# Impure: modifies external (global) variable\n",
    "result = 0\n",
    "def impure_square(x):\n",
    "    global result\n",
    "    result = x * x  # changes external state!\n",
    "    return result\n",
    "\n",
    "\n",
    "### Example 2: Modifying input arguments\n",
    "\n",
    "\n",
    "# Pure: creates a new list, does not modify input\n",
    "def append_pure(xs, x):\n",
    "    return xs + [x]\n",
    "\n",
    "# Impure: modifies the input list in-place\n",
    "def append_impure(xs, x):\n",
    "    xs.append(x)\n",
    "    return xs\n",
    "\n",
    "\n",
    "### Example 3: Printing, randomness, I/O\n",
    "\n",
    "\n",
    "# Pure: no printing or randomness\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Impure: prints a message (side effect)\n",
    "def add_print(a, b):\n",
    "    print(f\"Adding {a} and {b}\")\n",
    "    return a + b\n",
    "\n",
    "# Impure: uses randomness (output can change each call)\n",
    "import random\n",
    "def add_random(a):\n",
    "    return a + random.randint(0, 10)\n",
    "\n",
    "### Example 4: JAX and side effects\n",
    "\n",
    " \n",
    "\n",
    "# Pure: always same result, no print, no mutation\n",
    "def jax_pure_fn(x):\n",
    "    return jnp.sin(x) * 2\n",
    "\n",
    "# Impure: not allowed in JAX transforms (prints inside function)\n",
    "def jax_impure_fn(x):\n",
    "    print(\"Calculating!\")\n",
    "    return jnp.sin(x) * 2\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896da93-800a-41b8-9ab8-a581a05aaa11",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Key Point:**  \n",
    "> JAX requires your functions to be pure for transformations like `jit`, `grad`, `vmap` to work correctly.  \n",
    "> If you use print, mutate inputs, or access global state, your code may break or give wrong results!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7933324-3bd4-45ed-8c35-a9e92465feb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
