{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38137a75-fa4b-413e-9be6-060832d23b17",
   "metadata": {},
   "source": [
    "# JAX for Scientific Machine Learning: From Basics to PINNs and Operator Learning\n",
    "\n",
    "**Audience:** Master's-level interns with basic Python and ML background  \n",
    "**Tools:** JAX, Haiku/Flax/Equinox (for NN), Optax (for optimizers), Matplotlib/Plotly (for plotting)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial will guide you from the foundations of JAX and neural networks to advanced applications in scientific ML, specifically Physics-Informed Neural Networks (PINNs) and Operator Learning (DeepONet, FNO).  \n",
    "Each module includes readings, code exercises, and milestones.\n",
    "\n",
    "---\n",
    "\n",
    "## Module 1: JAX Basics\n",
    "\n",
    "**Objective:** Understand JAX's array programming, automatic differentiation, and functional programming style.\n",
    "\n",
    "### Readings:\n",
    "- [JAX 101: The Sharp Bits](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html)\n",
    "- [JAX: Autograd and XLA](https://docs.jax.dev/en/latest/advanced-autodiff.html#advanced-autodiff)\n",
    "- [Quickstart to other advanced concepts of jax](https://docs.jax.dev/en/latest/quickstart.html#)\n",
    "\n",
    "### Key Concepts:\n",
    "- `jax.numpy` vs. `numpy`\n",
    "- Pure functions\n",
    "- `grad`, `jit`, `vmap`, `pmap`\n",
    "- Static vs. dynamic arrays\n",
    "\n",
    "### Assignment 1:\n",
    "- Implement and plot the gradient of a univariate function (e.g., `f(x) = sin(x) + x^2`) using JAX's `grad`.\n",
    "- Compare the result with finite differences.\n",
    "\n",
    "---\n",
    "\n",
    "## Module 2: Building Neural Networks in JAX\n",
    "\n",
    "**Objective:** Learn to build and train neural networks using JAX libraries.\n",
    "\n",
    "### Readings:\n",
    "- [The Flax Philosophy](https://flax-linen.readthedocs.io/en/latest/philosophy.html)\n",
    "- [Haiku: Basic Tutorial](https://dm-haiku.readthedocs.io/en/latest/)\n",
    "- [Equinox: Simple neural nets](https://docs.kidger.site/equinox/all-of-equinox/)\n",
    "\n",
    "### Key Concepts:\n",
    "- Parameter initialization and management\n",
    "- Forward function\n",
    "- Loss computation\n",
    "- Parameter updates using Optax\n",
    "\n",
    "### Assignment 2:\n",
    "- Implement a fully-connected neural network for 1D regression (e.g., fit `f(x) = sin(3x)`) in Flax/Haiku/Equinox.\n",
    "- Visualize training loss and predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Module 3: Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "**Objective:** Understand and implement PINNs for solving differential equations.\n",
    "\n",
    "### Readings:\n",
    "- [Original PINN paper (Raissi et al.)](https://www.sciencedirect.com/science/article/pii/S0021999118307125)\n",
    "- [PINN overview (Distill-style)](https://maziarraissi.github.io/PINNs/)\n",
    " \n",
    "\n",
    "### Key Concepts:\n",
    "- Defining PDE residuals with automatic differentiation\n",
    "- Collocation and boundary points\n",
    "- Loss as a sum of physics and boundary losses\n",
    " \n",
    "---\n",
    "\n",
    "## Module 4: Operator Learning (DeepONet, FNO)\n",
    "\n",
    "**Objective:** Learn neural operators that map functions to functions, a step beyond standard PINNs.\n",
    "\n",
    "### Readings:\n",
    "- [DeepONet: Theory and Applications](https://www.nature.com/articles/s42256-021-00302-5)\n",
    "- [Fourier Neural Operator (FNO) paper](https://arxiv.org/abs/2010.08895)\n",
    "- [U-FNO](https://arxiv.org/abs/2109.03697)\n",
    "\n",
    "### Key Concepts:\n",
    "- Neural operators: learning mappings between infinite-dimensional spaces\n",
    "- Training on families of PDEs or parametric equations\n",
    "- Encoder-branch/trunk structure (DeepONet), spectral convolution (FNO)\n",
    "\n",
    " \n",
    "\n",
    "## Recommended Additional Readings\n",
    "\n",
    "- [Neural Operators: Survey](https://arxiv.org/abs/2108.08481)\n",
    "- [Physics-Informed Machine Learning Review](https://arxiv.org/abs/2308.08468)\n",
    "- [JAX Scientific ML resources](https://github.com/google/jax#scientific-machine-learning)\n",
    "\n",
    "---\n",
    "\n",
    "## Tips\n",
    "\n",
    "- Use Google Colab or JAX on GPU for faster training.\n",
    "- Keep all code modular and functions pure for easier debugging and JAX compatibility.\n",
    "- For further research, explore [Equinox](https://github.com/patrick-kidger/equinox) and [Diffrax](https://github.com/patrick-kidger/diffrax) for advanced scientific ML with JAX.\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ad215-3fec-4ca3-b0a4-50da38402cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
